{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POVa - Facial recognition from RGB-Depth images (identification)\n",
    "\n",
    "Our goal is to get some face recognition working using RGB-D data (e.g. Kinect).\n",
    "\n",
    "- Detect faces in images using an existing detector. Good choices are OpenCV, Dlib or MTCNN https://github.com/DCGM/mtcnn.\n",
    "- Align the face based on detected facial features (map to an average face).\n",
    "- Optional: (Try align 3D face pose using the depth data)\n",
    "- Train a neural network to identify faces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "dataset_name = \"atulanandjha/lfwpeople\"\n",
    "DATA_PATH = \"../data/\" + dataset_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run only if you don't have the dataset already in your project.\n",
    "________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "\n",
    "path = kagglehub.dataset_download(dataset_name)\n",
    "print(\"\\n\"+path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%mkdir -p $DATA_PATH\n",
    "%mv $path/* $DATA_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar -xvzf $DATA_PATH/*.tgz -C $DATA_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import shutil\n",
    "\n",
    "if not os.path.isdir(\"../data/dataset/\"):\n",
    "    os.mkdir(\"../data/dataset/\")\n",
    "    os.mkdir(\"../data/dataset/training\")\n",
    "    os.mkdir(\"../data/dataset/test\")\n",
    "\n",
    "for base_path, dirs, _ in os.walk(\"../data/\" + dataset_name + \"/lfw_funneled\"):\n",
    "    for dir in dirs:\n",
    "        if len(os.listdir(os.path.join(base_path, dir))) >= 5:\n",
    "            if not os.path.isdir(os.path.join(\"../data/dataset/training\", dir)):\n",
    "                os.mkdir(os.path.join(\"../data/dataset/training\", dir))\n",
    "    \n",
    "            file_list = os.listdir(os.path.join(base_path, dir))\n",
    "            random.shuffle(file_list)\n",
    "    \n",
    "            split = int(0.8 * len(file_list))\n",
    "    \n",
    "            for file_name in file_list[0:split]:\n",
    "                full_img_path = os.path.join(base_path, dir, file_name)\n",
    "                if os.path.isfile(full_img_path):\n",
    "                    shutil.copy(full_img_path, os.path.join(\"../data/dataset/training\", dir))\n",
    "        \n",
    "            for file_name in file_list[split:]:\n",
    "                full_img_path = os.path.join(base_path, dir, file_name)\n",
    "                if os.path.isfile(full_img_path):\n",
    "                    shutil.copy(full_img_path, \"../data/dataset/test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LFW - People (Face Recognition) Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://vis-www.cs.umass.edu/lfw/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install mtcnn tensorflow keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mtcnn import MTCNN\n",
    "import cv2\n",
    "\n",
    "detector = MTCNN()\n",
    "image = cv2.imread(\"../data/dataset/training/Abdullah_Gul/Abdullah_Gul_0001.jpg\")\n",
    "faces = detector.detect_faces(image)\n",
    "for face in faces:\n",
    "    box = face['box']\n",
    "    cv2.rectangle(image, (box[0], box[1]), (box[0]+box[2], box[1]+box[3]), (255, 0, 0), 2)\n",
    "cv2.imshow(\"Detected Faces\", image)\n",
    "cv2.waitKey(0) # Escape key\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the number of different classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_labels = 0\n",
    "for root, dirs, files in os.walk(DATA_PATH + \"/lfw_funneled/\"):\n",
    "    n_labels += len(dirs)\n",
    "    break\n",
    "\n",
    "print(\"Number of classes: \", n_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Face Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_lfw_people\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "lfw_people = fetch_lfw_people(min_faces_per_person=40, resize=1.0)\n",
    "X = lfw_people.images\n",
    "y = lfw_people.target\n",
    "target_names = lfw_people.target_names\n",
    "\n",
    "n_samples, width, height = lfw_people.images.shape\n",
    "n_features = X.shape[1]\n",
    "n_classes = target_names.shape[0]\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total dataset size:\")\n",
    "print(\"n_samples: %d\" % n_samples)\n",
    "print(\"n_features: %d\" % n_features)\n",
    "print(\"n_classes: %d\" % n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(1)\n",
    "figs, axes = plt.subplots(4, 6)\n",
    "for i in range(4):\n",
    "    for j in range(6): \n",
    "        axes[i, j].imshow(X[i*6+j,:,:])\n",
    "        axes[i, j].set_xticks([])\n",
    "        axes[i, j].set_yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# normalization\n",
    "x_train = x_train.astype('float32') / 255.0\n",
    "x_test  = x_test.astype( 'float32') / 255.0\n",
    "\n",
    "# convert integer labesls to categorical vectors\n",
    "y_train = to_categorical(y_train, n_labels)\n",
    "y_test  = to_categorical(y_test, n_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Facial Feature Alignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "epochs     = 100\n",
    "l_rate      = 2e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras import optimizers\n",
    "import tensorflow as tf\n",
    "\n",
    "def baseline(W=32, H=32, nclass=10, nchannel=3,lr=1e-4):\n",
    "    in1 = layers.Input(shape=(W, H, nchannel))\n",
    "    x = layers.Conv2D(32, (3, 3), strides=(1, 1),\n",
    "                      padding='valid',\n",
    "                      activation='relu')(in1)    \n",
    "    x = layers.MaxPool2D((2, 2))(x)\n",
    "    x = layers.BatchNormalization()(x)    \n",
    "    x = layers.Conv2D(32, (3, 3), strides=(1, 1),\n",
    "                      padding='valid',\n",
    "                      activation='relu')(x)    \n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPool2D((2, 2))(x)    \n",
    "    x = layers.Conv2D(64, (3, 3), strides=(1, 1),\n",
    "                      padding='valid',\n",
    "                      activation='relu')(x)\n",
    "    x = layers.MaxPool2D((2, 2))(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dropout(0.4)(x)\n",
    "    # x = layers.Dense(128, activation='relu')(x)\n",
    "    output = layers.Dense(nclass, activation='softmax')(x)\n",
    "    model = Model(inputs=in1, outputs=output)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=optimizers.Adam(learning_rate=lr),\n",
    "                  metrics=['acc'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_filepath = './checkpoints/checkpoint.weights.h5'\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor='val_acc',\n",
    "    mode='max',\n",
    "    save_best_only=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_base = baseline(W=width, H=height, nclass=n_labels ,nchannel=1, lr=l_rate)\n",
    "model_base.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model_base.fit(x_train, y_train, epochs=epochs, batch_size=batch_size, callbacks=[model_checkpoint_callback], validation_split=0.2, verbose=False)\n",
    "\n",
    "model_base.load_weights(checkpoint_filepath)   \n",
    "test_loss, test_acc = model_base.evaluate(x_test, y_test)    \n",
    "print('test acc for model_base: ', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss'] \n",
    "\n",
    "plt.figure(2)\n",
    "epochs = range(len(acc))\n",
    "plt.plot(epochs, acc, 'r', label='Training acc - baseline')\n",
    "plt.plot(epochs, val_acc, 'm:', label='Validation acc - baseline')\n",
    "\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(3)\n",
    "plt.plot(epochs, loss, 'r',  label='Training loss - baseline')\n",
    "plt.plot(epochs, val_loss, 'm:', label='Validation loss - baseline')\n",
    "plt.title( 'Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
